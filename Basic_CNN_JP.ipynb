{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Basic CNN Tutorial\n",
    "\n",
    "For this Jupyter notebook you will learn how to build basic CNN model and how can train your self model. \n",
    "\n",
    "**Requirement**\n",
    "* Pytroch 1.8.1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import Library"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch \n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, models, transforms\n",
    "from matplotlib import pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "model_name = \"Deepfake_Model_with_CNN\"\n",
    "layers = 50\n",
    "batch_size = 8\n",
    "train_path = \"./Datasets/\"\n",
    "output_path = \"./output/\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparing Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Data transforms setting    \n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Folder exist or not\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "# cuDNN nn model optimzation\n",
    "torch.backends.cudnn.benchmark= True"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Load training dataset\n",
    "train_dataset = torchvision.datasets.ImageFolder(train_path, transform = data_transforms['train'])\n",
    "# print(train_dataset.class_to_idx)\n",
    "\n",
    "# Split training dataset\n",
    "train_size = int(0.7 * len(train_dataset))\n",
    "valid_size = len(train_dataset) - train_size\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(train_dataset, [train_size, valid_size])\n",
    "# print(\"Num of Classes: \" +  str(len(.classes)))\n",
    "print(\"Training Size: \" + str(train_size))\n",
    "print(\"Training Size: \" + str(valid_size))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training Size: 8647\n",
      "Training Size: 3706\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Creat dataloader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size= batch_size, shuffle= True, drop_last= False, num_workers= 8)\n",
    "val_loader = torch.utils.data.DataLoader(valid_dataset, batch_size= batch_size, shuffle= True, drop_last= False, num_workers= 8)\n",
    "\n",
    "print(train_loader)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x1053431f0>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Construct The Standard CNN \n",
    "\n",
    "This model have 6 layers.\n",
    "\n",
    "### Function Explaining\n",
    "\n",
    "nn.Conv2d\n",
    "torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n",
    "\n",
    "**tips: stride = 1, padding = (kernel_size-1)/2 = (5-1)/2**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class Basic_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Basic_CNN, self).__init__()\n",
    "        \"\"\"\n",
    "        Image size (256, 256)\n",
    "        Input convolution (3, 256, 256)\n",
    "        Conv1 output (8, 128, 128)\n",
    "        Conv2 output (8, 64, 64)\n",
    "        Conv3 output (16, 32, 32)\n",
    "        Conv4 output (16, 8, 8)\n",
    "        \"\"\"\n",
    "        self.conv1 = nn.Conv2d(3, 8, 3, padding= 1)\n",
    "        self.conv2 = nn.Conv2d(8, 8, 5, padding= 2)\n",
    "        self.conv3 = nn.Conv2d(8, 16, 5, padding= 2)\n",
    "        self.conv4 = nn.Conv2d(16, 16, 5, padding= 2)\n",
    "\n",
    "        self.act_conv = nn.ReLU(inplace= True)\n",
    "        self.act_fc = nn.LeakyReLU(0.1)\n",
    "\n",
    "        self.bn8 = nn.BatchNorm2d(8)\n",
    "        self.bn16 = nn.BatchNorm2d(16)\n",
    "\n",
    "        self.mp2 = nn.MaxPool2d(kernel_size= (2, 2))\n",
    "        self.mp4 = nn.MaxPool2d(kernel_size= (4, 4))\n",
    "\n",
    "        self.drop = nn.Dropout2d(0.5)\n",
    "\n",
    "        self.fc1 = nn.Linear(16*8*8, 16)\n",
    "        self.fc2 = nn.Linear(16, 2)\n",
    "\n",
    "        self.log = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = self.conv1(input)\n",
    "        x = self.act_conv(x)\n",
    "        x = self.bn8(x)\n",
    "        x = self.mp2(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.act_conv(x)\n",
    "        x = self.bn8(x)\n",
    "        x = self.mp2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.act_conv(x)\n",
    "        x = self.bn16(x)\n",
    "        x = self.mp2(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.act_conv(x)\n",
    "        x = self.bn16(x)\n",
    "        x = self.mp4(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.act_fc(x)\n",
    "        \n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.log(x)\n",
    "    \n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def train(model, n_epochs, train_loader, valid_loader, optimizer, criterion):\n",
    "    train_acc_his, valid_acc_his= [],[]\n",
    "    train_losses_his, valid_losses_his= [],[]\n",
    "    # train_on_gpu = True\n",
    "    for epoch in range(n_epochs):\n",
    "        # keep track of training and validation loss\n",
    "        train_loss, valid_loss = 0.0, 0.0\n",
    "        train_losses, valid_losses= [],[]\n",
    "        train_correct, val_correct,train_total, val_total= 0,0,0,0\n",
    "        train_pred, train_target= torch.zeros(8,1), torch.zeros(8,1)\n",
    "        val_pred, val_target= torch.zeros(8,1), torch.zeros(8,1)\n",
    "        count= 0\n",
    "        count2= 0\n",
    "        print('running epoch: {}'.format(epoch + 1))\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        # model.cuda()\n",
    "        model.train()\n",
    "        for (data, target) in tqdm(train_loader):\n",
    "            # print(target)\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            # if train_on_gpu:\n",
    "            #     data, target= data.cuda(), target.cuda()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            #calculate accuracy\n",
    "            pred= output.data.max(dim= 1, keepdim= True)[1]\n",
    "            train_correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "            train_total += data.size(0)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update training loss\n",
    "            train_losses.append(loss.item() * data.size(0))\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            if count == 0:\n",
    "                train_pred=pred\n",
    "                train_target=target.data.view_as(pred)\n",
    "                count= count+1\n",
    "            else:\n",
    "                train_pred= torch.cat((train_pred,pred), 0)\n",
    "                train_target= torch.cat((train_target,target.data.view_as(pred)), 0)\n",
    "        train_pred=train_pred.cpu().view(-1).numpy().tolist()\n",
    "        train_target=train_target.cpu().view(-1).numpy().tolist()\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        for (data, target) in tqdm(valid_loader):\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            # if train_on_gpu:\n",
    "            #     data, target = data.cuda(), target.cuda()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss =criterion(output, target)\n",
    "            #calculate accuracy\n",
    "            pred = output.data.max(dim = 1, keepdim = True)[1]\n",
    "            val_correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "            val_total += data.size(0)\n",
    "            valid_losses.append(loss.item()*data.size(0))\n",
    "            if count2==0:\n",
    "                val_pred=pred\n",
    "                val_target=target.data.view_as(pred)\n",
    "                count2=count+1\n",
    "            else:\n",
    "                val_pred = torch.cat((val_pred,pred), 0)\n",
    "                val_target = torch.cat((val_target,target.data.view_as(pred)), 0)\n",
    "        val_pred=val_pred.cpu().view(-1).numpy().tolist()\n",
    "        val_target=val_target.cpu().view(-1).numpy().tolist()\n",
    "        \n",
    "        # calculate average losses\n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        \n",
    "        # calculate average accuracy\n",
    "        train_acc = train_correct/train_total\n",
    "        valid_acc = val_correct/val_total\n",
    "        train_acc_his.append(train_acc)\n",
    "        valid_acc_his.append(valid_acc)\n",
    "        train_losses_his.append(train_loss)\n",
    "        valid_losses_his.append(valid_loss)\n",
    "        # print training/validation statistics \n",
    "        print('\\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            train_loss, valid_loss))\n",
    "        print('\\tTraining Accuracy: {:.6f} \\tValidation Accuracy: {:.6f}'.format(\n",
    "            train_acc, valid_acc))\n",
    "    return train_acc_his, valid_acc_his, train_losses_his, valid_losses_his, model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "n_epochs= 100\n",
    "model = Basic_CNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001, betas = (0.9, 0.999), eps = 1e-08)\n",
    "train_acc_his, valid_acc_his, train_losses_his, valid_losses_his, model = train(model, n_epochs, train_loader, val_loader, optimizer, criterion)\n",
    "torch.save(model, output_path + \"Deepfake_Model_with_CNN.pt\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "running epoch: 1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 33%|███▎      | 354/1081 [2:01:10<4:08:51, 20.54s/it]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/v5/nbbk089j7d371hzqzg8grth40000gn/T/ipykernel_7469/210240035.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-08\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_acc_his\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc_his\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_losses_his\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_losses_his\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/v5/nbbk089j7d371hzqzg8grth40000gn/T/ipykernel_7469/1542843574.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, n_epochs, train_loader, valid_loader, optimizer, criterion)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mtrain_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;31m# backward pass: compute gradient of the loss with respect to model parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0;31m# perform a single optimization step (parameter update)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.subplot(221)\n",
    "plt.plot(train_losses_his, 'b', label = 'training loss')\n",
    "plt.plot(valid_losses_his, 'r', label = 'validation loss')\n",
    "plt.title(\"ResNet Loss\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.subplot(222)\n",
    "plt.plot(train_acc_his, 'b', label = 'trainingaccuracy')\n",
    "plt.plot(valid_acc_his, 'r', label = 'validation accuracy')\n",
    "plt.title(\"ResNet Accuracy\")\n",
    "plt.legend(loc = 'upper left')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}